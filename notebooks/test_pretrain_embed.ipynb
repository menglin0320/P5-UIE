{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "from packaging import version\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from src.param import parse_args\n",
    "from src.utils import LossMeter\n",
    "from src.dist_utils import reduce_dict\n",
    "from transformers import T5Tokenizer, T5TokenizerFast\n",
    "from src.tokenization import P5Tokenizer, P5TokenizerFast\n",
    "from src.pretrain_model import P5Pretraining\n",
    "\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "\n",
    "# Check if Pytorch version >= 1.6 to switch between Native AMP and Apex\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "from src.trainer_base import TrainerBase\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path,'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "args = DotDict()\n",
    "\n",
    "args.distributed = False\n",
    "args.multiGPU = True\n",
    "args.fp16 = True\n",
    "args.train = \"beauty\"\n",
    "args.valid = \"beauty\"\n",
    "args.test = \"beauty\"\n",
    "args.batch_size = 16\n",
    "args.optim = 'adamw' \n",
    "args.warmup_ratio = 0.05\n",
    "args.lr = 1e-3\n",
    "args.num_workers = 4\n",
    "args.clip_grad_norm = 1.0\n",
    "args.losses = 'rating,sequential,explanation,review,traditional'\n",
    "args.backbone = 't5-small' # small or base\n",
    "args.output = 'snap/beauty-small'\n",
    "args.epoch = 10\n",
    "args.local_rank = 0\n",
    "\n",
    "args.comment = ''\n",
    "args.train_topk = -1\n",
    "args.valid_topk = -1\n",
    "args.dropout = 0.1\n",
    "\n",
    "args.tokenizer = 'p5'\n",
    "args.max_text_length = 512\n",
    "args.do_lower_case = False\n",
    "args.word_mask_rate = 0.15\n",
    "args.gen_max_length = 64\n",
    "\n",
    "args.weight_decay = 0.01\n",
    "args.adam_eps = 1e-6\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.continuous_embed = True\n",
    "'''\n",
    "Set seeds\n",
    "'''\n",
    "args.seed = 2022\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "'''\n",
    "Whole word embedding\n",
    "'''\n",
    "args.whole_word_embed = True\n",
    "\n",
    "cudnn.benchmark = True\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.world_size = ngpus_per_node\n",
    "\n",
    "LOSSES_NAME = [f'{name}_loss' for name in args.losses.split(',')]\n",
    "if args.local_rank in [0, -1]:\n",
    "    print(LOSSES_NAME)\n",
    "LOSSES_NAME.append('total_loss') # total loss\n",
    "\n",
    "args.LOSSES_NAME = LOSSES_NAME\n",
    "\n",
    "gpu = 0 # Change GPU ID\n",
    "args.gpu = gpu\n",
    "args.rank = gpu\n",
    "print(f'Process Launching at GPU {gpu}')\n",
    "\n",
    "torch.cuda.set_device('cuda:{}'.format(gpu))\n",
    "\n",
    "comments = []\n",
    "dsets = []\n",
    "if 'toys' in args.train:\n",
    "    dsets.append('toys')\n",
    "if 'beauty' in args.train:\n",
    "    dsets.append('beauty')\n",
    "if 'sports' in args.train:\n",
    "    dsets.append('sports')\n",
    "comments.append(''.join(dsets))\n",
    "if args.backbone:\n",
    "    comments.append(args.backbone)\n",
    "comments.append(''.join(args.losses.split(',')))\n",
    "if args.comment != '':\n",
    "    comments.append(args.comment)\n",
    "comment = '_'.join(comments)\n",
    "\n",
    "if args.local_rank in [0, -1]:\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(args):\n",
    "    from transformers import T5Config, BartConfig\n",
    "\n",
    "    if 't5' in args.backbone:\n",
    "        config_class = T5Config\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    config = config_class.from_pretrained(args.backbone)\n",
    "    config.dropout_rate = args.dropout\n",
    "    config.dropout = args.dropout\n",
    "    config.attention_dropout = args.dropout\n",
    "    config.activation_dropout = args.dropout\n",
    "    config.losses = args.losses\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def create_tokenizer(args):\n",
    "    from transformers import T5Tokenizer, T5TokenizerFast\n",
    "    from src.tokenization import P5Tokenizer, P5TokenizerFast\n",
    "\n",
    "    if 'p5' in args.tokenizer:\n",
    "        tokenizer_class = P5Tokenizer\n",
    "\n",
    "    tokenizer_name = args.backbone\n",
    "    \n",
    "    if args.continuous_embed:\n",
    "        datamaps = load_json(os.path.join('../data', args.train, 'datamaps.json'))\n",
    "        user_extra_ids = len(datamaps['user2id']) + 1 # index start from 1\n",
    "        item_extra_ids = len(datamaps['item2id']) + 1 \n",
    "    else:\n",
    "        user_extra_ids = 0\n",
    "        item_extra_ids = 0\n",
    "    tokenizer = P5Tokenizer.from_pretrained(\n",
    "            args.backbone, \n",
    "            max_length=args.max_text_length, \n",
    "            do_lower_case=args.do_lower_case,\n",
    "            user_extra_ids=user_extra_ids,\n",
    "            item_extra_ids=item_extra_ids\n",
    "            )\n",
    "\n",
    "    print(tokenizer_class, tokenizer_name)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def create_model(model_class, config=None):\n",
    "    print(f'Building Model at GPU {args.gpu}')\n",
    "\n",
    "    model_name = args.backbone\n",
    "\n",
    "    model = model_class.from_pretrained(\n",
    "        model_name,\n",
    "        config=config\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = create_config(args)\n",
    "\n",
    "if args.tokenizer is None:\n",
    "    args.tokenizer = args.backbone\n",
    "    \n",
    "tokenizer = create_tokenizer(args)\n",
    "\n",
    "model_class = P5Pretraining\n",
    "model = create_model(model_class, config)\n",
    "\n",
    "model = model.cuda()\n",
    "if 'p5' in args.tokenizer:\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "    \n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.load = \"../snap/beauty-small/Epoch10.pth\"\n",
    "args.load = \"../snap/pretrain_item/Epoch30.pth\"\n",
    "# Load Checkpoint\n",
    "from src.utils import load_state_dict, LossMeter, set_global_logging_level\n",
    "from pprint import pprint\n",
    "\n",
    "def load_checkpoint(ckpt_path):\n",
    "    state_dict = load_state_dict(ckpt_path, 'cpu')\n",
    "    results = model.load_state_dict(state_dict, strict=False)\n",
    "    print('Model loaded from ', ckpt_path)\n",
    "    pprint(results)\n",
    "\n",
    "ckpt_path = args.load\n",
    "print(torch.norm(torch.mean(model.get_input_embeddings().weight.data[:model.tokenizer.sp_model.get_piece_size()], 0)))\n",
    "\n",
    "load_checkpoint(ckpt_path)\n",
    "print(torch.norm(torch.mean(model.get_input_embeddings().weight.data[model.tokenizer.sp_model.get_piece_size() + model.tokenizer._extra_ids:model.tokenizer.sp_model.get_piece_size() + model.tokenizer._extra_ids+model.tokenizer._user_extra_ids], 0)))\n",
    "print(torch.norm(torch.mean(model.get_input_embeddings().weight.data[model.tokenizer.sp_model.get_piece_size() + model.tokenizer._extra_ids -4:], 0)))\n",
    "\n",
    "from src.all_amazon_templates import all_tasks as task_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from src.pretrain_data import get_loader\n",
    "from evaluate.utils import rouge_score, bleu_score, unique_sentence_percent, root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
    "from evaluate.metrics4rec import evaluate_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_task_list = {'relate_inout_embeds': ['6-1'] # or '1-6'\n",
    "}\n",
    "test_sample_numbers = {'learn_title': 1, 'relate_inout_embeds': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='train', \n",
    "        tokenizer = tokenizer,\n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "gt_ratings = []\n",
    "pred_ratings = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        print(batch['source_text'][0])\n",
    "        print('goal: ', batch['target_text'][0])\n",
    "        print('res: ', results[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_task_list = {'learn_title': ['7-1'] # or '1-6'\n",
    "}\n",
    "test_sample_numbers = {'learn_title': 1, 'relate_inout_embeds': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='train', \n",
    "        tokenizer = tokenizer,\n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "gt_ratings = []\n",
    "pred_ratings = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        print(batch['source_text'][0])\n",
    "        print('goal: ', batch['target_text'][0])\n",
    "        print('res: ', results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_task_list = {'learn_category': ['8-1'] # or '1-6'\n",
    "}\n",
    "test_sample_numbers = {'learn_title': 1, 'relate_inout_embeds': 1, 'learn_category': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='train', \n",
    "        tokenizer = tokenizer,\n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "gts = []\n",
    "preds = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        gts.extend(batch['target_text'])\n",
    "        preds.extend(results)\n",
    "        # print(batch['source_text'][0])\n",
    "        # print('goal: ', batch['target_text'][0])\n",
    "        # print('res: ', results[0])\n",
    "accy = np.mean(np.asarray(gts) == np.asarray(preds))\n",
    "print(accy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_task_list = {'learn_brand': ['9-1'] # or '1-6'\n",
    "}\n",
    "test_sample_numbers = {'learn_title': 1, 'relate_inout_embeds': 1, 'learn_brand': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='train', \n",
    "        tokenizer = tokenizer,\n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "gts = []\n",
    "preds = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        gts.extend(batch['target_text'])\n",
    "        preds.extend(results)\n",
    "        print(batch['source_text'][0])\n",
    "        print('goal: ', batch['target_text'][0])\n",
    "        print('res: ', results[0])\n",
    "accy = np.mean(np.asarray(gts) == np.asarray(preds))\n",
    "print(accy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66566\n",
      "Data sources:  ['beauty']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'learn_price'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m test_task_list \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlearn_price\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39m11-1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# or '1-6'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m }\n\u001b[1;32m      3\u001b[0m test_sample_numbers \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlearn_title\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrelate_inout_embeds\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlearn_user\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m}\n\u001b[0;32m----> 5\u001b[0m zeroshot_test_loader \u001b[39m=\u001b[39m get_loader(\n\u001b[1;32m      6\u001b[0m         args,\n\u001b[1;32m      7\u001b[0m         test_task_list,\n\u001b[1;32m      8\u001b[0m         test_sample_numbers,\n\u001b[1;32m      9\u001b[0m         split\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtest, \n\u001b[1;32m     10\u001b[0m         mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     11\u001b[0m         tokenizer \u001b[39m=\u001b[39;49m tokenizer,\n\u001b[1;32m     12\u001b[0m         batch_size\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m     13\u001b[0m         workers\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mnum_workers,\n\u001b[1;32m     14\u001b[0m         distributed\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mdistributed\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(zeroshot_test_loader))\n\u001b[1;32m     18\u001b[0m gts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/projects/P5_with_embedding/P5/notebooks/../src/pretrain_data.py:1921\u001b[0m, in \u001b[0;36mget_loader\u001b[0;34m(args, task_list, sample_numbers, tokenizer, split, mode, batch_size, workers, distributed)\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1920\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mall_amazon_templates\u001b[39;00m \u001b[39mimport\u001b[39;00m all_tasks \u001b[39mas\u001b[39;00m task_templates\n\u001b[0;32m-> 1921\u001b[0m     dataset \u001b[39m=\u001b[39m P5_Amazon_Dataset(\n\u001b[1;32m   1922\u001b[0m         task_templates,\n\u001b[1;32m   1923\u001b[0m         task_list,\n\u001b[1;32m   1924\u001b[0m         tokenizer,\n\u001b[1;32m   1925\u001b[0m         args,\n\u001b[1;32m   1926\u001b[0m         sample_numbers,\n\u001b[1;32m   1927\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1928\u001b[0m         split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m   1929\u001b[0m         rating_augment\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1930\u001b[0m     )\n\u001b[1;32m   1932\u001b[0m \u001b[39mif\u001b[39;00m distributed:\n\u001b[1;32m   1933\u001b[0m     sampler \u001b[39m=\u001b[39m DistributedSampler(dataset)\n",
      "File \u001b[0;32m~/projects/P5_with_embedding/P5/notebooks/../src/pretrain_data.py:138\u001b[0m, in \u001b[0;36mP5_Amazon_Dataset.__init__\u001b[0;34m(self, all_tasks, task_list, tokenizer, args, sample_numbers, mode, split, rating_augment, sample_type)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatum_info \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 138\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_datum_info()\n",
      "File \u001b[0;32m~/projects/P5_with_embedding/P5/notebooks/../src/pretrain_data.py:218\u001b[0m, in \u001b[0;36mP5_Amazon_Dataset.compute_datum_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m     curr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_length\n\u001b[1;32m    217\u001b[0m \u001b[39melif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlearn_price\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_length \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_item) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_numbers[key]\n\u001b[1;32m    219\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_length \u001b[39m-\u001b[39m curr):\n\u001b[1;32m    220\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatum_info\u001b[39m.\u001b[39mappend((i \u001b[39m+\u001b[39m curr, key, i \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_numbers[key]))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'learn_price'"
     ]
    }
   ],
   "source": [
    "test_task_list = {'learn_user': ['11-1'] # or '1-6'\n",
    "}\n",
    "test_sample_numbers = {'learn_title': 1, 'relate_inout_embeds': 1, 'learn_user': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='train', \n",
    "        tokenizer = tokenizer,\n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "gts = []\n",
    "preds = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        gts.extend(batch['target_text'])\n",
    "        preds.extend(results)\n",
    "        print(batch['source_text'][0])\n",
    "        print('goal: ', batch['target_text'][0])\n",
    "        print('res: ', results[0])\n",
    "accy = np.mean(np.asarray(gts) == np.asarray(preds))\n",
    "print(accy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
